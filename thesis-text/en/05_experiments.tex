\chapter{Experiments}
\label{experiments_chap}

\section{Blackbox PoC on Google Cloud Vision API}
Here we go into more technical details about previously mentioned blackbox attacks we initially tried.

\begin{itemize}
    \item TREMBA
    \item RayS
    \item SquareAttack
    \item Sparse-RS
\end{itemize}

\subsection{Baseline}
We picked two sample images (shark and cat), on which we tested these blackbox attacks.

\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{cute_cat.png}
\endminipage
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{shark.png}
\endminipage
\caption{Cat and Shark, GVision baseline}
\label{fig:cat_shark_original}
\end{figure}


Here's how \href{https://cloud.google.com/vision}{Google Cloud Vision API} classifies the original samples.

\begin{figure}[!htb]
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{cute_cat_screenshot.png}
\endminipage\hfill
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{shark_screenshot.png}
%   \caption{$\epsilon = 0.05$}\label{fig:fgsm_margin_eps_05}
\endminipage\hfill
\caption{Cat and Shark, GVision baseline}
\label{fig:cat_shark_gvision_baseline}
\end{figure}

\subsection{TREMBA}

\subsection{RayS}
This one is hard label attack and doesn't use the continuos loss from GVision.

\subsection{SquareAttack}
Because of high query intensity, we have only tested the "cat" sample image.


\subsubsection{SquareAttack L2}
\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_l2_7.8_iter_834.png}
\endminipage
\null\hspace{1cm}
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_l2_7.8_iter_834_screenshot.png}
\endminipage
\caption{SquareAttack, 834 queries, pertubation norm $l_2 = 7.84$ }
\label{fig:square_cat_l2}
\end{figure}

\subsubsection{SquareAttack Linf}

\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_linf_0.047_iter_1402.png}
\endminipage
\null\hspace{1cm}
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_linf_0.047_iter_1402_screenshot.png}
\endminipage
\caption{SquareAttack, 1402 queries, pertubation norm $l_{inf} = 0.047$}
\label{fig:square_cat_linf}
\end{figure}

\subsubsection{Evaluation of the GVision SquareAttack results}

In both L2 and Linf modes we have achived out top-1 misclassification objective and the cat label was taken in both cases to the top-3 place.

In this experiment of sample size = 1 the L2 version of SquareAttack seems to produce much less visually perceptible pertubation and is able to achive the top-1 misclassification with $l_2$ norm of only 7.84.

The Linf version on the other hand had to be given two times larger query budget, but we still had to turn up the $l_{inf}$ pertubation norm to 0.047 to achieve our misclassification objective.


This observation made us focus more on the $l_2$ bounded attacks in later local experiments (\ref{local_transfer}).

\subsubsection{Local query distribution}
To get an idea how SquareAttack would fare against some of our local models, we ran it in low query mode with 200 queries against ResNet-18 and ResNet-50, and with 300 queries against EfficientNet-b0 and it's adversarially trained counterpart. To make the job a bit easier for the SquareAttack, we relaxed the $l_2$ pertubation to 20.

\begin{figure}[!htb]
\centering
 \subfloat[SquareAttack L2 on ResNets]{\includegraphics[width=\linewidth]{n_queries_needed/square_attack_l2_eps_20.png}}\hfill
 \subfloat[SquareAttack L2 on EfficientNets]{\includegraphics[width=\linewidth]{n_queries_needed/square_attack_efficientnet_vs_efficientnet_advtrain.png}}\hfill
\caption{SquareAttack L2 on local models}
\end{figure}

What is maybe a little bit surprising is that the adversarial training of EfficientNet-b0 makes no difference when attacked by blackbox SquareAttack. This may be in part attributed to the fact, that the adversarial training defence primarily flattens out the gradients around training input data (\cite{Yu2018TowardsRT}), but doesn't provide any robustness guarantees (\cite{Kolter2018ProvableDA}). As the blackbox SquareAttack doesn't use the gradient information explicitly, it may not be as affected by the adversarial training defense as the whitebox graidient attacks.


\section{Local transferability experiments}
\label{local_transfer}
Motivated by the not-ideal query-efficiency of pure-blackbox attacks (\ref{need_for_query_efficiency}), we moved to transfer attacks to see how far we can push the pure transfer threat model.


\subsection{Choice of local models}
We performed all our experiments on the following pretrained PyTorch ImageNet models.

\begin{itemize}
    \item ResNet-18, ResNet-50 (\cite{he2015deep})
    \item ResNeXt-50 (32x4d) (\cite{xie2017aggregated})
    \item Wide-ResNet-50-2 (\cite{zagoruyko2017wide})
    \item Squeezenet (\cite{iandola2016squeezenet})
    \item DenseNet-121 (\cite{huang2018densely})
    \item EfficientNet (\cite{tan2020efficientnet})
    \item EfficientNet adversarially trained (\cite{tramer2020ensemble})
\end{itemize}

Apart from EfficientNets, all models were taken from the \href{https://pytorch.org/vision/stable/models.html}{torchvision.models} Python package. For the EfficientNets we used \href{https://github.com/lukemelas/EfficientNet-PyTorch}{github.com/lukemelas/EfficientNet-PyTorch} reimplementation, because the original implementation uses Tensorflow, and PyTorch is just so much better than Tensorflow.

To address the particular choice of our model set, we aimed at low model size, such that forward and backward pass fits comfortably in the 2GB of our MX-150 NVIDIA laptop GPU. This requirement for instance ruled out the VGG-style networks (\cite{simonyan2015deep}), the pre-ResNet 2nd-best submission to the ILSVRC 2014 (\cite{russakovsky2015imagenet}). The Wide-ResNet-50-2 (WRN-50-2) is already at the limit with its 68,951,464 trainable parameters. In a standard FP-32 mode the model parameters, forward pass activations and backward pass partial derivatives take up almost 1GB of GPU memory. Adding to this the 620MB of GPU memory consumed by PyTorch at idle meant we could use only batch size of one for this particular Wide ResNet. But looking at the \href{https://robustbench.github.io/}{RobustBench leaderboard} (\cite{croce2021robustbench}), which uses AutoAttack (\cite{croce2020reliable}) suite of parameter-free attacks to benchmark the robustness of various adversarially robust models, one can see the top positions are dominated by WideResNets, so we kept it in our model set despite its large size.
We also chose models with the same input size, such that the same dataset preprocessing (\ref{dataset_preprocessing}) could be used for all of them and wouldn't complicate things any further. This decision meant we didn't use the popular Inception-v3 (\cite{szegedy2015rethinking}), which takes inputs of size 299x299 instead of 224x224 of all the other models.

\subsubsection{Inference time}
In the early days of AdvPipe we had supported only batch sizes of one. We wondered what kind of difference does batch size make in such a memory-limited hardware setting. As we can see in the following figure, having batches much larger than you hardware can handle (ResNet-101, ResNet-152) can result in a sub-par performance when compared to $bs=1$. We think that this slow-down is caused by the frequent re-allocations the Cuda-backend has to make during the forward pass (and also during the backward pass).

\begin{figure}[!htb]
  \includegraphics[width=\linewidth]{inference_time/inference_resnet_and_efficient_net.png}
\caption{ResNet vs EfficientNet inference}
\label{fig:fgsm_margin}
\end{figure}



\subsection{Choice of dataset}
We carry out all our experiments with ImageNet validation dataset. We pick out only the images containing organism using the mapping $m(c)$ mentioned in \ref{label_mapping} and deem the transfer attack successful if the target loss $\mathcal{L}_{margin}(x_{adv}, 0) < 0$, or in other words the top-1 label is an object label.

For the computation limitations reasons, each experiment was conducted with only the first 500 organism ImageNet validation images.

\subsubsection{Dataset preprocessing}
\label{dataset_preprocessing}
All the models we use accept input tensors of size $(\text{batch\_size}, 3, 224, 224)$ and they also share the same preprocessing procedure:
\begin{verbatim}
from torchvision import transforms
transform = transforms.Compose([
 transforms.Resize(256),
 transforms.CenterCrop(224),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
 ])
\end{verbatim}

Usually, this preprocessing is handled dynamically either by the dataloader or by the model itself. The user doesn't have to Resize, Crop or Normalize the inputs manually, but can pass in images of any size for inference or training.

However to speed up the experiments a bit we manually resized and center croped all the ImageNet val images, such that no dynamic resizing and cropping is needed. This saves up a surprising amount computation on our limited hardware.


\subsection{Baseline}
\begin{tabular}{ ||p{5cm}|p{3cm}|| }
  \hline
     \multicolumn{2}{|c|}{Performance on the first 500 ImageNet validation organism images} \\
 \hline
 \textbf{Model} & \textbf{Foolrate} \\
 \hline
 \hline
 Densenet-121 & 1.8\%  \\
 \hline
 EfficientNet-b0 & 1.0\% \\
 \hline
 EfficientNet-b0-advtrain & 1.2\% \\
 \hline
 EfficientNet-b4 & 0.8\% \\
 \hline
 EfficientNet-b4-advtrain & 2.4\% \\
 \hline
 ResNet-18 & 1.8\% \\
 \hline
 ResNet-50 & 1.2\% \\
 \hline
 ResNeXt-50 (32x4d) & 0.6\% \\
 \hline
 Squeezenet & 4.8\% \\
 \hline
 Wide-ResNet-50-2 & 1.6\% \\
 \hline
\end{tabular}

\paragraph{}
We can see that the models used are pretty good at distinguishing between animate and inanimate things. Any differances in the accuracy (maybe with the exception of Squeezenet) are probably due to the small test set size. When we evaluate 500 test images and get 1\% foolrate the 99\% binomial confidence interval is (0.216\%, 2.804\%)

\subsection{Whitebox attack algorithms}
In the whitebox setting we tried the following whitebox optimization algorithms:

\begin{itemize}
    \item FGSM (\cite{goodfellow2015explaining})
    \item Auto-PGD (APGD) L2 (\cite{croce2020reliable})
    \item AdamPDG L2
\end{itemize}


\subsubsection{Fast gradient sign method (FGSM)}
This algorithm needs no introduction.
It is basically one-step gradient ascent on the sign of the cross-entropy loss $J(x)$, which would be normally used to train the classifier.

$$x_{adv} = clip(x + \epsilon \cdot sign(\nabla J(x)))$$


Careful reader might wonder: How do we compute a cross-entropy, when our local model doesn't output logits, but uses the max-logits mapping from \ref{computing_the_loss} instead? Well, we don't. We just pretend our surrogate is outputing excatly what it should and everything is fine. Figure \ref{fig:fgsm_margin} shows the transferability performance across different local models with varying amount of pertubation size $\epsilon$.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.010.png}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.050.png}
%   \caption{$\epsilon = 0.05$}\label{fig:fgsm_margin_eps_05}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.200.png}
%   \caption{$\epsilon = 0.2$}\label{fig:fgsm_margin_eps_20}
\endminipage
\caption{FGSM, margin output mapping}
\label{fig:fgsm_margin}
\end{figure}

To test the impact of different logits mapping function, we also try to pass the logits from the pretrained ImageNet classifier through the $softmax$, sum the two sets of probabilities, take the logarithm and pass that to the FGSM as an attempt to better mimick some binary animal-object classifier. The motivation here was to allow the gradient to pass through more than 2 output activations of the 1000-D ImageNet output vector, hopefullly to provide more fine-grained optimization information to the one step of FGSM.

But the figure \ref{fig:fgsm_prob_sum} shows that this approach is approximately equal in transferability to the margin mapping, and for some unknown reason fails to produce whitebox adversarial images for the EfficientNets.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.010.png}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.050.png}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.200.png}
\endminipage
\caption{FGSM, probability sum mapping}
\label{fig:fgsm_prob_sum}
\end{figure}


\subsubsection{Underfitting vs. overfitting}
\label{fgsm_underfits}
FGSM experiment showed that one gradient optimization step isn't quite enough, because ideally we would like to see the whitebox foolrate (the diagonal in the heatmaps) being close to 100\%. If we were to use the analogy from \ref{ml_analogy}, the diagonal would correspond to the training performance. Anything off the diagonal can be though of as validation performance. The training accuracy of FGSM is already quite low, so we cannot expect the validation accuracy to be much higher. In other words, FGSM massively underfits. It is the extreme case of early-stopping, which in the classical machine learning is sometimes used to prevent overfitting (\cite{Caruana2000OverfittingIN}).

\subsubsection{The need for a better optimizer}
We argued in \ref{fgsm_underfits} that FGSM underfits badly even on adversarially undefended whitebox networks with large pertubation budget. What we need here is a stronger optimizer.

There have been proposed numerous iterative whitebox attack algorithms that are doing in one form or another a gradient descent (or ascent) on the adversarial loss. Just to name a few:

\begin{itemize}
    \item Basic iterative method - BIM (\cite{Kurakin2017AdversarialEI})
    \item Projected gradient descent - PGD (\cite{madry2019deep})
    \item Momentum iterative fast gradient sign method - MI-FGSM (\cite{Dong2018BoostingAA})
    \item Nesterov Iterative Fast Gradient Sign Method - NI-FGSM (\cite{lin2020nesterov})
    \item Auto-PGD - APGD (\cite{croce2020reliable})
    \item Adam Iterative Fast Gradient Method - AI-FGM (\cite{Yin2021BoostingAA})
\end{itemize}

Out of those optimization methods we really liked the APGD, because unlike the others, it is hyperparmeter free.

The only knobs to we can tweak are:

\begin{itemize}
    \item pertubation size
    \item number of gradient step iterations
    \item number of gradient samples (when dealing with non-deterministic models)
\end{itemize}

Actually it is so good at optimizing the adversarial loss, that in our experiments the output probability of the organism class would often go exactly to zero. After computing the cross-entropy loss, which involves taking the logarithm of this probability, we would be getting infinities in the objective and the subsequent gradients would become $NaN$.

\subsubsection{APGD baseline}
To establish the baseline performance of APGD and its ability to optimize our custom binary classification loss, we tried a number of different $l_2$ norm budgets and executed it with 25 iterations, which is the lowest number of gradient steps the authors use in their comparisions APGD to classical PGD (\cite{croce2020reliable}). We used this low-end of the $N_{iters}$ APGD boundary to keep the experiment running times under control.

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.33\textwidth]{transfer_attacks/transfer_heatmap/apgd_l2_margin_n_iters_25_eps_3.png}}
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.33\textwidth]{transfer_attacks/transfer_heatmap/apgd_l2_margin_n_iters_25_eps_10.png}}
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.33\textwidth]{transfer_attacks/transfer_heatmap/apgd_l2_margin_n_iters_25_eps_20.png}}
\caption{APGD L2 baseline}
\label{fig:apgd_baseline}
\end{figure}



What is obvious is that the 25 iterations did its work and took the training loss to zero. The whitebox foolrate is now 100\% in almost all cases, even when the pertubations are small ($l_2 = 3$). While succeeding on the diagonal, the biggest problem now is the overfitting to one particular surrogate.


Figures \ref{fig:apgd_baseline_resnet18}, \ref{fig:apgd_baseline_resnet50} and \ref{fig:apgd_baseline_efficientnet-b0} show some examples of the adversarial images produced:

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_3/surrogate_resnet18/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_10/surrogate_resnet18/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_20/surrogate_resnet18/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}
\caption{APGD L2 baseline ResNet-18}
\label{fig:apgd_baseline_resnet18}
\end{figure}

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_3/surrogate_resnet50/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_10/surrogate_resnet50/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_20/surrogate_resnet50/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}
\caption{APGD L2 baseline ResNet-50}
\label{fig:apgd_baseline_resnet50}
\end{figure}

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.32\textwidth]{apgd_baseline/eps_3/surrogate_efficientnet-b0-advtrain/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.32\textwidth]{apgd_baseline/eps_10/surrogate_efficientnet-b0-advtrain/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.32\textwidth]{apgd_baseline/eps_20/surrogate_efficientnet-b0-advtrain/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}
\caption{APGD L2 baseline EffecientNet-b0-advtrain}
\label{fig:apgd_baseline_efficientnet-b0}
\end{figure}

It is interesting to see how the adversarially trained EfficientNet-b0 is more sensitive to the dog's nose and mouth area in the dog's image, while the ResNets produce pertubations that are much more uniform.


\subsection{Augmentation is all you need!}
Seeing how little transferability there is between the models from the ResNet family and the EfficientNets, we have started experimenting with some image augmentations techniques, which are commonly used to to prevent overfitting and enhance generalization when training normal CNN classifiers. 
 We have tried augmenting images with:


\begin{itemize}
    \item Guassian-noise
    \item Blur
    \item Elastic transformation
    \item Affine transformation
\end{itemize}


\subsubsection{Guassian-noise augmentation}

\cite{Wu2020TowardsUA} show how convolving the loss surface with a Gaussian filter has a smoothing effect on the gradient.

$$J_{\sigma}(x) = \mathbb{E}_{\xi \sim \mathcal{N}(0, I)}[J(x + \sigma \xi)]$$
$$\nabla J_{\sigma}(x) = \mathbb{E}_{\xi \sim \mathcal{N}(0, I)}[\nabla J(x + \sigma \xi)]$$

In their experiments they visualize the saliency map of the gradient $\nabla J_{\sigma}(x)$ with increasing values of $\sigma$.
They go on to show how increasing the values of $\sigma$ filters out the noise in the gradient significantly while still capturing the most significant semantic information. 

We confirm this observation in our experiments. In figure (\ref{fig:apgd_noise_smooth_gradient}) we demonstrate how inceasing the $\sigma$ compels the algorithm to focus only on a smaller part of the image. It cannot afford to spread the pertubation around the image uniformly, because low-intensity signal is destroyed by the Guassian noise. It must produce a pertubation in a smaller area but with higher intensity, to keep the pertubation's signal-to-noise ratio high.


\begin{figure}
\centering
 \subfloat[$\sigma = 2$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/2/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 4$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/4/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 8$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/8/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

 \subfloat[$\sigma = 12$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/12/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 16$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/16/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 20$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/20/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

 \subfloat[$\sigma = 24$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/24/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 28$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/28/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 32$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/32/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

\caption{Guassian noise dog augmentation sample}
\label{fig:apgd_noise_smooth_gradient}
\end{figure}


Figure \ref{fig:apgd_noise_augment} shows that in the low iteration APGD regime the optimal Guassian-noise augmentation std is somewhere near $\sigma=18$. It also shows that given the same whitebox query budget, it pays off to get a better quality gradient estimate.

TODO: add the citation of the paper, that shows the Guassian noise with std=16 to be pretty much optimal.

\begin{figure}
\centering
 \subfloat[25 iterations, 10 gradient estimates per step]{\includegraphics[width=0.7\textwidth]{transfer_attacks/transfer_heatmap/augment_noise_apgd_l2_margin_n_iters_25_eot_iters_10_eps_10.png}}

 \subfloat[250 iterations, 1 gradient estimate per step]{\includegraphics[width=0.7\textwidth]{transfer_attacks/transfer_heatmap/augment_noise_apgd_l2_margin_n_iters_250_eot_iters_1_eps_10.png}}
\caption{Guassian noise augmentation}
\label{fig:apgd_noise_augment}
\end{figure}

\subsubsection{Box-blur}
In the next experiment (figure \ref{fig:apgd_box_blur}) we test the effect of box-blur on adeversarial-image transferability. Box-blur of size 'x' applies uniform convolution kernel of size 'x with normalized weights over the input image. 

\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/augment_blur_apgd_l2_margin_n_iters_50_eps_10.png}
    \caption{APGD box-blur augmentation}
    \label{fig:apgd_box_blur}
\end{figure}

We can witness that a small amout of blur is beneficial, but larger amounts are too strong regularizers for the whitebox optimization and cause the APGD optimization algorithm to underfit.

\subsubsection{Elastic transformation}
We have also tried to regularize our optimization with stochastic elastic-transformation (figure \ref{fig:apgd_elastic}). This particular transformation had proved to be a powerful regularization element, which brought the transferability from ResNet-18 to EfficientNet-b0-advtrain to 54.0\%.


TODO: describe in more detail what exactly it's the elastic transform...
\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/augment_elastic_apgd_l2_margin_n_iters_25_eot_iters_10_eps_10.png}
    \caption{APGD elastic transformation augmentations}
    \label{fig:apgd_elastic}
\end{figure}

\subsubsection{Affine transformation}

\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/augment_affine_n_iters_30_eot_iters_10_eps_10.png}
    \caption{APGD elastic transformation augmentations}
    \label{fig:apgd_elastic}
\end{figure}


\subsubsection{Ensemble}
TODO:


\section{Transferability evaluation on GVision}
TODO: just run the images against GVision

\subsection{Choice of evaluation metrics}

\subsection{Wordcloud}