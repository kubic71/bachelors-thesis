\chapter{Experiments}
\label{experiments_chap}

\section{Blackbox PoC on Google Cloud Vision API}
Here we go into more technical details about previously mentioned blackbox attacks we initially tried.

\begin{itemize}
    \item TREMBA
    \item RayS
    \item SquareAttack
    \item Sparse-RS
\end{itemize}

\subsection{Baseline}
We picked two sample images (shark and cat), on which we tested these blackbox attacks.

\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{cute_cat.png}
\endminipage
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{shark.png}
\endminipage
\caption{Cat and Shark, GVision baseline}
\label{fig:cat_shark_original}
\end{figure}


Here's how \href{https://cloud.google.com/vision}{Google Cloud Vision API} classifies the original samples.

\begin{figure}[!htb]
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{cute_cat_screenshot.png}
\endminipage\hfill
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{shark_screenshot.png}
%   \caption{$\epsilon = 0.05$}\label{fig:fgsm_margin_eps_05}
\endminipage\hfill
\caption{Cat and Shark, GVision baseline}
\label{fig:cat_shark_gvision_baseline}
\end{figure}

\subsection{TREMBA}

\subsection{RayS}
This one is hard label attack and doesn't use the continuos loss from GVision.

\subsection{SquareAttack}
Because of high query intensity, we have only tested the "cat" sample image.


\subsubsection{SquareAttack L2}
\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_l2_7.8_iter_834.png}
\endminipage
\null\hspace{1cm}
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_l2_7.8_iter_834_screenshot.png}
\endminipage
\caption{SquareAttack, 834 queries, pertubation norm $l_2 = 7.84$ }
\label{fig:square_cat_l2}
\end{figure}

\subsubsection{SquareAttack Linf}

\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_linf_0.047_iter_1402.png}
\endminipage
\null\hspace{1cm}
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_linf_0.047_iter_1402_screenshot.png}
\endminipage
\caption{SquareAttack, 1402 queries, pertubation norm $l_{inf} = 0.047$}
\label{fig:square_cat_linf}
\end{figure}

\subsubsection{Evaluation of the GVision SquareAttack results}

In both L2 and Linf modes we have achived out top-1 misclassification objective and the cat label was taken in both cases to the top-3 place.

In this experiment of sample size = 1 the L2 version of SquareAttack seems to produce much less visually perceptible pertubation and is able to achive the top-1 misclassification with $l_2$ norm of only 7.84.

The Linf version on the other hand had to be given two times larger query budget, but we still had to turn up the $l_{inf}$ pertubation norm to 0.047 to achieve our misclassification objective.


This observation made us focus more on the $l_2$ bounded attacks in later local experiments (\ref{local_transfer}).

\subsubsection{Local query distribution}
To get an idea how SquareAttack would fare against some of our local models, we ran it in low query mode with 200 queries against ResNet-18 and ResNet-50, and with 300 queries against EfficientNet-b0 and it's adversarially trained counterpart. To make the job a bit easier for the SquareAttack, we relaxed the $l_2$ pertubation to 20.

\begin{figure}[!htb]
 \subfloat[SquareAttack L2 on ResNets]{\includegraphics[width=\linewidth]{n_queries_needed/square_attack_l2_eps_20.png}}\hfill
 \subfloat[SquareAttack L2 on EfficientNets]{\includegraphics[width=1.12\linewidth]{n_queries_needed/square_attack_efficientnet_vs_efficientnet_advtrain.png}}\hfill
\caption{SquareAttack L2 on local models}
\end{figure}

What is maybe a little bit surprising is that the adversarial training of EfficientNet-b0 makes no difference when attacked by blackbox SquareAttack. This may be in part attributed to the fact, that the adversarial training defence primarily flattens out the gradients around training input data (\cite{Yu2018TowardsRT}), but doesn't provide any robustness guarantees (\cite{Kolter2018ProvableDA}). As the blackbox SquareAttack doesn't use the gradient information explicitly, it may not be as affected by the adversarial training defense as the whitebox gradient attacks.


\section{Local transferability experiments}
\label{local_transfer}
Motivated by the not-ideal query-efficiency of pure-blackbox attacks (\ref{need_for_query_efficiency}), we moved to transfer attacks to see how far we can push the pure transfer threat model.


\subsection{Choice of local models}
We performed all our experiments on the following pretrained PyTorch ImageNet models.

\begin{itemize}
    \item ResNet-18, ResNet-50 (\cite{he2015deep})
    \item ResNeXt-50 (32x4d) (\cite{xie2017aggregated})
    \item Wide-ResNet-50-2 (\cite{zagoruyko2017wide})
    \item Squeezenet (\cite{iandola2016squeezenet})
    \item DenseNet-121 (\cite{huang2018densely})
    \item EfficientNet (\cite{tan2020efficientnet})
    \item EfficientNet adversarially trained (\cite{tramer2020ensemble})
\end{itemize}

Apart from EfficientNets, all models were taken from the \href{https://pytorch.org/vision/stable/models.html}{torchvision.models} Python package. For the EfficientNets we used \href{https://github.com/lukemelas/EfficientNet-PyTorch}{github.com/lukemelas/EfficientNet-PyTorch} reimplementation, because the original implementation uses Tensorflow, and PyTorch is just so much better than Tensorflow.

To address the particular choice of our model set, we aimed at low model size, such that forward and backward pass fits comfortably in the 2GB of our MX-150 NVIDIA laptop GPU. This requirement for instance ruled out the VGG-style networks (\cite{simonyan2015deep}), the pre-ResNet 2nd-best submission to the ILSVRC 2014 (\cite{russakovsky2015imagenet}). The Wide-ResNet-50-2 (WRN-50-2) is already at the limit with its 68,951,464 trainable parameters. In a standard FP-32 mode the model parameters, forward pass activations and backward pass partial derivatives take up almost 1GB of GPU memory. Adding to this the 620MB of GPU memory consumed by PyTorch at idle meant we could use only batch size of one for this particular Wide ResNet. But looking at the \href{https://robustbench.github.io/}{RobustBench leaderboard} (\cite{croce2021robustbench}), which uses AutoAttack (\cite{croce2020reliable}) suite of parameter-free attacks to benchmark the robustness of various adversarially robust models, one can see the top positions are dominated by WideResNets, so we kept it in our model set despite its large size.
We also chose models with the same input size, such that the same dataset preprocessing (\ref{dataset_preprocessing}) could be used for all of them and wouldn't complicate things any further. This decision meant we didn't use the popular Inception-v3 (\cite{szegedy2015rethinking}), which takes inputs of size 299x299 instead of 224x224 of all the other models.

\subsubsection{Inference time}
In the beginning, AdvPipe had supported only batch sizes of one. In \ref{fig:inference_time} it can be seen what kind of difference does batch size make on the inference time in such a memory-limited hardware setting. The small batch sizes didn't slow down the running times more than twice. We can also see in the figure that having batches much larger than you hardware can handle (ResNet-101, ResNet-152) can result in a sub-par performance when compared to $bs=1$. We think that this slow-down is caused by the frequent re-allocations the Cuda-backend has to make during the forward pass (and also during the backward pass).

\begin{figure}[!htb]
  \includegraphics[width=\linewidth]{inference_time/inference_resnet_and_efficient_net.png}
\caption{ResNet vs EfficientNet inference}
\label{fig:inference_time}
\end{figure}



\subsection{Choice of dataset}
We carry out all our experiments with ImageNet validation dataset. We pick out only the images containing organism using the mapping $m(c)$ mentioned in \ref{label_mapping} and deem the transfer attack successful if the target loss $\mathcal{L}_{margin}(x_{adv}, 0) < 0$, or in other words the top-1 label is an object label.

For the computation limitations reasons, each experiment was conducted with only the first 500 organism ImageNet validation images.

\subsubsection{Dataset preprocessing}
\label{dataset_preprocessing}
All the models we use accept input tensors of size $(\text{batch\_size}, 3, 224, 224)$ and they also share the same preprocessing procedure:
\begin{verbatim}
from torchvision import transforms
transform = transforms.Compose([
 transforms.Resize(256),
 transforms.CenterCrop(224),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
 ])
\end{verbatim}

Usually, this preprocessing is handled dynamically either by the dataloader or by the model itself. The user doesn't have to Resize, Crop or Normalize the inputs manually, but can pass in images of any size for inference or training.

However to speed up the experiments a bit we manually resized and center croped all the ImageNet val images, such that no dynamic resizing and cropping is needed. This saves up a surprising amount computation on our limited hardware.


\subsection{Baseline}
\begin{tabular}{ ||p{5cm}|p{3cm}|| }
  \hline
     \multicolumn{2}{|c|}{Performance on the first 500 ImageNet validation organism images} \\
 \hline
 \textbf{Model} & \textbf{Foolrate} \\
 \hline
 \hline
 Densenet-121 & 1.8\%  \\
 \hline
 EfficientNet-b0 & 1.0\% \\
 \hline
 EfficientNet-b0-advtrain & 1.2\% \\
 \hline
 EfficientNet-b4 & 0.8\% \\
 \hline
 EfficientNet-b4-advtrain & 2.4\% \\
 \hline
 ResNet-18 & 1.8\% \\
 \hline
 ResNet-50 & 1.2\% \\
 \hline
 ResNeXt-50 (32x4d) & 0.6\% \\
 \hline
 Squeezenet & 4.8\% \\
 \hline
 Wide-ResNet-50-2 & 1.6\% \\
 \hline
\end{tabular} \\ \\

We can see that the models used are pretty good at distinguishing between animate and inanimate things. Any differances in the accuracy (maybe with the exception of Squeezenet) are probably due to the small test set size. When we evaluate 500 test images and get 1\% foolrate the 99\% binomial confidence interval is (0.216\%, 2.804\%)

\subsection{Whitebox attack algorithms}
In the whitebox setting we tried the following whitebox optimization algorithms:

\begin{itemize}
    \item FGSM (\cite{goodfellow2015explaining})
    \item Auto-PGD (APGD) L2 (\cite{croce2020reliable})
    \item AdamPDG L2
\end{itemize}


\subsubsection{Fast gradient sign method (FGSM)}
Fast gradient sign method is basically one-step gradient ascent on the sign of the cross-entropy loss $J(x)$, which would be normally used to train the classifier.

$$x_{adv} = clip(x + \epsilon \cdot sign(\nabla J(x)))$$


Careful reader might wonder: How do we compute a cross-entropy, when our local model doesn't output logits, but uses the max-logits mapping from \ref{computing_the_loss} instead? Well, we don't. We just pretend our surrogate is outputing excatly what it should and everything is fine. Figure \ref{fig:fgsm_margin} shows the transferability performance across different local models with varying amount of pertubation size $\epsilon$.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.010.png}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.050.png}
%   \caption{$\epsilon = 0.05$}\label{fig:fgsm_margin_eps_05}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.200.png}
%   \caption{$\epsilon = 0.2$}\label{fig:fgsm_margin_eps_20}
\endminipage
\caption{FGSM, margin output mapping}
\label{fig:fgsm_margin}
\end{figure}

To test the impact of different logits mapping function, we also try to pass the logits from the pretrained ImageNet classifier through the $softmax$, sum the two sets of probabilities, take the logarithm and pass that to the FGSM as an attempt to better mimick some binary animal-object classifier. The motivation here was to allow the gradient to pass through more than 2 output activations of the 1000-D ImageNet output vector, hopefullly to provide more fine-grained optimization information to the one step of FGSM.

But the figure \ref{fig:fgsm_prob_sum} shows that this approach is approximately equal in transferability to the margin mapping, and for some unknown reason fails to produce whitebox adversarial images for the EfficientNets.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.010.png}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.050.png}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.200.png}
\endminipage
\caption{FGSM, probability sum mapping}
\label{fig:fgsm_prob_sum}
\end{figure}


\subsubsection{Underfitting vs. overfitting}
\label{fgsm_underfits}
FGSM experiment showed that one gradient optimization step isn't quite enough, because ideally we would like to see the whitebox foolrate (the diagonal in the heatmaps) being close to 100\%. If we were to use the analogy from \ref{ml_analogy}, the diagonal would correspond to the training performance. Anything off the diagonal can be though of as validation performance. The training accuracy of FGSM is already quite low, so we cannot expect the validation accuracy to be much higher. In other words, FGSM massively underfits. It is the extreme case of early-stopping, which in the classical machine learning is sometimes used to prevent overfitting (\cite{Caruana2000OverfittingIN}).

\subsubsection{The need for a better optimizer}
We argued in \ref{fgsm_underfits} that FGSM underfits badly even on adversarially undefended whitebox networks with large pertubation budget. What we need here is a stronger optimizer.

There have been proposed numerous iterative whitebox attack algorithms that are doing in one form or another a gradient descent (or ascent) on the adversarial loss. Just to name a few:

\begin{itemize}
    \item Basic iterative method - BIM (\cite{Kurakin2017AdversarialEI})
    \item Projected gradient descent - PGD (\cite{madry2019deep})
    \item Momentum iterative fast gradient sign method - MI-FGSM (\cite{Dong2018BoostingAA})
    \item Nesterov Iterative Fast Gradient Sign Method - NI-FGSM (\cite{lin2020nesterov})
    \item Auto-PGD - APGD (\cite{croce2020reliable})
    \item Adam Iterative Fast Gradient Method - AI-FGM (\cite{Yin2021BoostingAA})
\end{itemize}

Out of those optimization methods we really liked the APGD, because unlike the others, it is hyperparmeter free.

The only knobs to we can tweak are:

\begin{itemize}
    \item pertubation size
    \item number of gradient step iterations
    \item number of gradient samples (when dealing with non-deterministic models)
\end{itemize}

Actually it is so good at optimizing the adversarial loss, that in our experiments the output probability of the organism class would often go exactly to zero. After computing the cross-entropy loss, which involves taking the logarithm of this probability, we would be getting infinities in the objective and the subsequent gradients would become $NaN$.

\subsubsection{APGD baseline}
To establish the baseline performance of APGD and its ability to optimize our custom binary classification loss, we tried a number of different $l_2$ norm budgets and executed it with 25 iterations, which is the lowest number of gradient steps the authors use in their comparisions APGD to classical PGD (\cite{croce2020reliable}). We used this low-end of the $N_{iters}$ APGD boundary to keep the experiment running times under control.

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.33\textwidth]{transfer_attacks/transfer_heatmap/apgd_l2_margin_n_iters_25_eps_3.png}}
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.33\textwidth]{transfer_attacks/transfer_heatmap/apgd_l2_margin_n_iters_25_eps_10.png}}
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.33\textwidth]{transfer_attacks/transfer_heatmap/apgd_l2_margin_n_iters_25_eps_20.png}}
\caption{APGD L2 baseline}
\label{fig:apgd_baseline}
\end{figure}



What is obvious is that the 25 iterations did its work and took the training loss to zero. The whitebox foolrate is now 100\% in almost all cases, even when the pertubations are small ($l_2 = 3$). While succeeding on the diagonal, the biggest problem now is the overfitting to one particular surrogate.


Figures \ref{fig:apgd_baseline_resnet18}, \ref{fig:apgd_baseline_resnet50} and \ref{fig:apgd_baseline_efficientnet-b0} show some examples of the adversarial images produced:

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_3/surrogate_resnet18/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_10/surrogate_resnet18/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_20/surrogate_resnet18/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}
\caption{APGD L2 baseline ResNet-18}
\label{fig:apgd_baseline_resnet18}
\end{figure}

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_3/surrogate_resnet50/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_10/surrogate_resnet50/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.3\textwidth]{apgd_baseline/eps_20/surrogate_resnet50/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}
\caption{APGD L2 baseline ResNet-50}
\label{fig:apgd_baseline_resnet50}
\end{figure}

\begin{figure}[!htb]
\centering
 \subfloat[$\epsilon = 3$]{\includegraphics[width=0.32\textwidth]{apgd_baseline/eps_3/surrogate_efficientnet-b0-advtrain/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
\subfloat[$\epsilon = 10$]{\includegraphics[width=0.32\textwidth]{apgd_baseline/eps_10/surrogate_efficientnet-b0-advtrain/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\epsilon = 20$]{\includegraphics[width=0.32\textwidth]{apgd_baseline/eps_20/surrogate_efficientnet-b0-advtrain/target_densenet-121/adv_examples/ILSVRC2012_val_00000003.png}}
\caption{APGD L2 baseline EffecientNet-b0-advtrain}
\label{fig:apgd_baseline_efficientnet-b0}
\end{figure}

It is interesting to see how the adversarially trained EfficientNet-b0 is more sensitive to the dog's nose and mouth area in the dog's image, while the ResNets produce pertubations that are much more uniform.


\subsection{Augmentation is all you need!}
Seeing how little transferability there is between the models from the ResNet family and the EfficientNets, we have started experimenting with some image augmentations techniques, which are commonly used to to prevent overfitting and enhance generalization when training normal CNN classifiers. 
 We have tried augmenting images with:


\begin{itemize}
    \item Guassian-noise
    \item Blur
    \item Elastic transformation
    \item Affine transformation
\end{itemize}


\subsubsection{Guassian-noise augmentation}

\cite{Wu2020TowardsUA} show how convolving the loss surface with a Gaussian filter has a smoothing effect on the gradient.

$$J_{\sigma}(x) = \mathbb{E}_{\xi \sim \mathcal{N}(0, I)}[J(x + \sigma \xi)]$$
$$\nabla J_{\sigma}(x) = \mathbb{E}_{\xi \sim \mathcal{N}(0, I)}[\nabla J(x + \sigma \xi)]$$

In their experiments they visualize the saliency map of the gradient $\nabla J_{\sigma}(x)$ with increasing values of $\sigma$.
They go on to show how increasing the values of $\sigma$ filters out the noise in the gradient significantly while still capturing the most significant semantic information. 

We confirm this observation in our experiments. In figure (\ref{fig:apgd_noise_smooth_gradient}) we demonstrate how increasing the $\sigma$ compels the optimization to focus only on a smaller part of the image. It cannot afford to spread the pertubation around the image uniformly, because low-intensity signal is destroyed by the Guassian noise. It must produce a pertubation in a smaller area but with higher intensity, to keep the pertubation's signal-to-noise ratio high.


\begin{figure}
\centering
 \subfloat[$\sigma = 2$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/2/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 4$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/4/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 8$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/8/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

 \subfloat[$\sigma = 12$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/12/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 16$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/16/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 20$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/20/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

 \subfloat[$\sigma = 24$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/24/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 28$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/28/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\sigma = 32$]{\includegraphics[width=0.32\textwidth]{results/augment_noise_sample/resnet18/32/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

\caption{Guassian noise augmentation ResNet-18}
\label{fig:apgd_noise_smooth_gradient}
\end{figure}


Because the surrogate is stochastic and during each forward pass applies a random transformation $t \sim T$, we actually optimize the expected surrogate loss in a Expectation over Transformation (EoT - \cite{athalye2018synthesizing}) style:

$$J_{T}(x) = \mathbb{E}_{t \sim T}[J(t(x))]$$
$$\nabla J_{T}(x) = \mathbb{E}_{t \sim T}[\nabla J(t(x))]$$

We estimate the true $\nabla J_T(x)$ using the sample mean estimator.

$$\widehat{\nabla} J_{T}(x) = \frac{1}{N_{EoT}} \sum_{i=1}^{N_{EoT}} \nabla J(t(x))$$

The $N_{EoT}$ is somewhat analogous to a batch size in SGD neural net training.

In figure \ref{fig:apgd_noise_augment} explore, whether this iterated sampling is even necessary, or whether the noise in the gradient could be handled by the APGD. We set a constant query bugdet of 250 and run the APGD in two configurations: $N_{EoT} = 1$ and $N_{EoT} = 10$. The results suggest that sampling the gradient only once is not enough, but it's hard to say what's the optimal $N_{EoT}$ given a constant total query budget. 

Intuitively, noisier gradients need more $N_{EoT}$ samples. If we take first order Taylor approximation of the loss - $J(x + \epsilon) \approx J(x) + \epsilon \nabla J(x)$, then:
$$std(\nabla J(x + \sigma \mathcal{N}(0, I))) = std(\nabla J(x) + \sigma  \mathcal{N}(0, I) \nabla^2 J(x)) = \sigma \nabla^2 J(x) = \sigma c$$

The standard deviation of the sample mean of the gradient would be:
$$std(\widehat{\nabla} J_{T}(x)) = std(\frac{1}{N_{EoT}} \sum_{i=1}^{N_{EoT}} \nabla J(x + \sigma \mathcal{N}(0, I)))=$$
$$ = \frac{1}{N_{EoT}} \sqrt{N_{EoT}} std(\nabla J(x + \sigma \mathcal{N}(0, I))) = \frac{1}{\sqrt{N_{EoT}}} \sigma c $$

So to keep the gradient estimate noisiness constant, we would need to scale $N_{EoT}$ quadratically with the gaussian noise's $\sigma$.

\begin{figure}
\centering
 \subfloat[25 iterations, 10 gradient estimates per step]{\includegraphics[width=0.7\textwidth]{transfer_attacks/transfer_heatmap/augment_noise_apgd_l2_margin_n_iters_25_eot_iters_10_eps_10.png}}

 \subfloat[250 iterations, 1 gradient estimate per step]{\includegraphics[width=0.7\textwidth]{transfer_attacks/transfer_heatmap/augment_noise_apgd_l2_margin_n_iters_250_eot_iters_1_eps_10.png}}
\caption{Guassian noise augmentation}
\label{fig:apgd_noise_augment}
\end{figure}


Figure \ref{fig:apgd_noise_augment} also shows that when the total query budget is set at 250, the optimal Guassian-noise augmentation $\sigma$ is somewhere near $\sigma=18$. 



This is in agreement with \cite{Wu2020TowardsUA}, where they find the distortion level $\sigma = 15$ to be performing the best.


To explore how much the additional gradient sampling helps, we have set $\sigma$ to an excessive $\sigma=35$ and ran experiments with $N_{EoT} \in \{1, 3, 10, 30, 100\}$.

Figure \ref{fig:noise_eot_iter} shows the foolrate steadily increasing up to $N_{EoT} = 100$. It is a reminder that we shouldn't underestimate the EoT gradient sampling when using very noisy stochastic augmentations, or when stacking severeal stochastic augmentations in the preprocessing pipeline of our surrogates, in which case the gradient noise is multiplied at each stage.


\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/noise_eot_iter_exp_resnet18.png}
    \caption{The effect of $N_{EoT}$ in noisy gradient situation}
    \label{fig:noise_eot_iter}
\end{figure}



\subsubsection{Box-blur}
In this experiment we have tested the effect of box-blur on adeversarial-image transferability. Box-blur augmentation applies uniform normalized convolution kernel of size $x$ over the input image. 

\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/augment_blur_apgd_l2_margin_n_iters_50_eps_10.png}
    \caption{APGD box-blur augmentation}
    \label{fig:apgd_box_blur}
\end{figure}

We can see in figure \ref{fig:apgd_box_blur} that a small amout of blur is beneficial, but larger amounts are essentially strong low-pass filters that introduce large surrogate bias. Large blur makes the surrogate a much weaker classifier, which is quite easy to fool at train-time, but when the blurring is removed from the preprocessing pipeline at test-time, the model becomes stronger again and isn't fooled by the adversarial image produced on the weak surrogate.

Figure \ref{fig:apgd_box_blur_sample} shows the effect of increasing the kernel size of the box-blur on the frequency of the adversarial pertubation. Bigger blurring kernels lower the pertubation frequency, because the high-freqency information cannot pass through the blur averaging effectively.


\begin{figure}
\centering
 \subfloat[kernel size = 1]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/1/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}\hfill
 \subfloat[kernel size = 2]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/2/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}\hfill
 \subfloat[kernel size = 3]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/3/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}

 \subfloat[kernel size = 4]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/4/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}\hfill
 \subfloat[kernel size = 5]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/5/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}\hfill
 \subfloat[kernel size = 7]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/7/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}

 \subfloat[kernel size = 10]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/10/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}\hfill
 \subfloat[kernel size = 15]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/15/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}\hfill
 \subfloat[kernel size = 20]{\includegraphics[width=0.32\textwidth]{results/augment_blur_sample/resnet18/20/target_resnet18/adv_examples/ILSVRC2012_val_00000001.png}}


\caption{Effect of box-blur kernel size on the frequency of adversarial pertubation}
\label{fig:apgd_box_blur_sample}
\end{figure}


\subsubsection{Elastic transformation}
\label{elastic_transform}
\cite{qiu2020fencebox} compare various stochastic image augmentations as an adversarial attack defense. If we turn this around, by making the crafted adversarial images robust to those defensive augmentations, we might achive better transferability to models with such stochastic defenses. On top of that, our adversarial images will be robust to common image manipulations, which might easily happen if the preprocessing of the target model differs from the surrogate.

This robustness might be quite important, as we have no information about the image preprocessing that happens in cloud target.

Inspired by the defensive augmentations suggested by this the paper, we've added stochastic elastic transformarmation to our augmentation toolbox, because it was one of the best defensive augmentations when attacked by iterative FGSM. But the combination of APGD together with gradient averaging was enough to break this surrogate defense and in turn produce robust adversarial images that really transfer.

Figure \ref{fig:apgd_elastic} proves the potential of this augmentation, as it brought the transferability from ResNet-18 to EfficientNet-b0-advtrain to 54.0\%, which is a significant improvement over the baseline 8\%. It also outperformed by a significantly margin the 22\% achieved by gaussian noise and the 16\% achieved by box-blur.

One disadvantage of it is that due to its non-linearity, the forward and backward pass is really expensive.


\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/augment_elastic_apgd_l2_margin_n_iters_25_eot_iters_10_eps_10.png}
    \caption{APGD elastic transformation augmentations}
    \label{fig:apgd_elastic}
\end{figure}


On figure \ref{fig:apgd_elastic_sample} we can see similar things happening as in \ref{fig:apgd_noise_smooth_gradient}. With stronger elastic deformations the optimization cannot just put its decieving pertubations anywhere. Noise-like pertubations produced by the baseline on ResNet-18 are no longer an option. Their highly-tuned overfit nature makes them fragile to small image manipulations. The strong elastic regularization forces the APGD to change or somehow cover the semantic information in the image instead. In this case this means focusing on the dog instead of the wall and grass behing. The pertubations also seem to be more spatially consistent. Another thing to point out is how diverse are the types of patterns created at different levels of distortion.

\begin{figure}
\centering
 \subfloat[$\alpha = 0$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/0/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\alpha = 0.2$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/0.2/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\alpha = 0.4$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/0.4/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

 \subfloat[$\alpha = 0.8$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/0.8/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\alpha = 1.6$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/1.6/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\alpha = 2.4$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/2.4/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

 \subfloat[$\alpha = 3.2$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/3.2/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\alpha = 4.0$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/4.0/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\alpha = 6.4$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/6.4/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}

 \subfloat[$\alpha = 9$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/9/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
 \subfloat[$\alpha = 12.8$]{\includegraphics[width=0.32\textwidth]{results/apgd_augment_elastic_sample/surrogate_resnet18/12.8/target_resnet18/adv_examples/ILSVRC2012_val_00000003.png}}\hfill
\caption{Elastic augmentation ResNet-18}
\label{fig:apgd_elastic_sample}
\end{figure}

\begin{figure}
\centering
 \subfloat[$\alpha = 0$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/0.0.png}}\hfill
 \subfloat[$\alpha = 0.2$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/0.2.png}}\hfill
 \subfloat[$\alpha = 0.4$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/0.4.png}}

 \subfloat[$\alpha = 0.8$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/0.8.png}}\hfill
 \subfloat[$\alpha = 1.6$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/1.6.png}}\hfill
 \subfloat[$\alpha = 2.4$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/2.4.png}}

 \subfloat[$\alpha = 3.2$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/3.2.png}}\hfill
 \subfloat[$\alpha = 4.0$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/4.0.png}}\hfill
 \subfloat[$\alpha = 6.4$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/6.4.png}}

 \subfloat[$\alpha = 9$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/9.0.png}}\hfill
 \subfloat[$\alpha = 12.8$]{\includegraphics[width=0.32\textwidth]{ae_samples/elastic_dog_gvision/12.8.png}}

\caption{Elastic dog from ResNet-18 to GVision}
\label{fig:elastic_dog_gvision}
\end{figure}


Out of curiosity we also ran these sample images against Google Vision (figure \ref{fig:elastic_dog_gvision}). To our surprise Google Vision already makes some mistakes, even when using such a weak ResNet-18 surrogate. All successfully misclasified examples had their pertubations focused on the head of the dog, which highlights the importance of focusing on the important semantic features in the image.


\subsubsection{Affine transformation}
\label{affine_transform}
Another augmentation defense coming on top in \cite{qiu2020fencebox} was a random affine transformation. Kornia library offers this type of augmentation as well so we ran a few experiments to compare it to the elastic augmentation in \ref{elastic_transform}. 

The Kornia's affine augmentation has many tunable parameters, all of which are given as ranges, that are uniformly sampled from.
\begin{itemize}
    \item scale - the amount of isotropic scaling
    \item rotation - the range of degrees of rotation
    \item shift - how much to translate the image 
    \item shear - the amount of shear in x and y direction
\end{itemize}


\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/augment_affine_n_iters_30_eot_iters_10_eps_10.png}
    \caption{APGD elastic transformation augmentations (TODO - remove the 'deg' param and format the captions better)}
    \label{fig:apgd_affine}
\end{figure}


These 4 parameters make the hyperparameter search space larger than what was the case for previous augmentation. Figure \ref{fig:apgd_affine} shows an incomplete grid-search over those 4 hyperparameters.

The best configuration turned out to be 30 degree shear and small random shift of 0.2. It even beat the elastic augmentation in its transferability to EfficientNets-b0-advtrain with foolrate of 56.2\%.

This crude experiment shows there is difinitely a large room for improvement. The shear probably substitutes the elastic deformation and shift makes sure that only the important parts of the image are modified, because only a portion of the image is visible after each random translation.

The unreasonable effectiveness of the random shift suggests that Random Sized Padding Affine (RSPA) augmentation (\cite{qiu2020fencebox}) might be worth trying in the future.

\subsubsection{Ensemble}
\label{ensemble}
To put it all together, we created an ensemble of Wide-ResNet-50-2, EfficientNet-b0 and Squeezenet with equal voting weights as our surrogate. Their scores were combined by summing their output logits. We chose these 3 models because they appear to be complemenary in their transfer performance. We've added the Guassian noise augmentation with $\sigma = 32$, box-blur with kernel size = 2, and the elastic augmentation with $\alpha = 3.2$ to the mix.

The 2 random augmentations in the pipeline meant higher noise in the gradient, so we've also increased the $N_{EoT}$ to 20 and 100 respectively.

\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{transfer_attacks/transfer_heatmap/ensemble_elastic_3.2_noise_32_blur_2.png}
    \caption{Ensemble with augmentations}
    \label{fig:ensemble_augment}
\end{figure}

Figure \ref{fig:ensemble_augment} shows the results, which are all above 90\% the $N_{EoT}=20$ version that compansates for the lower number of $N_{EoT}$ by more APGD iterations. The version with extensive gradient sampling was run only for 16 iterations, which still adds up to more total ensemble queries. The performance difference shouldn't be compared too much, because we were forced to stop the 16-iteration experiment only after 192 validation images.

The attachment \ref{ensemble_sample_images} contains a couple of randomly chosen example adversarial images. Some of them are really creepy (\ref{fig:bane_dog})

\begin{figure}
    \centering
    \includegraphics[width=0.60\textwidth]{ae_samples/bane.png}
    \caption{"Oh, you think darkness is your ally. But you merely adopted the dark; I was born in it, molded by it." - The Dark Knight Rises}
    \label{fig:bane_dog}
\end{figure}
\pagebreak


\section{Transferability evaluation on GVision}
TODO: just run the images against GVision

\subsection{Choice of evaluation metrics}

\subsection{Wordcloud}