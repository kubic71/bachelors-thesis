\chapter{Experiments}
\label{experiments_chap}

\section{Blackbox PoC on GVision}
Here we go into more technical details about previously mentioned blackbox attacks we initially tried.

\begin{itemize}
    \item TREMBA
    \item RayS
    \item SquareAttack
    \item Sparse-RS
\end{itemize}

\subsection{Baseline}
We picked two sample images (shark and cat), on which we tested these blackbox attacks.

\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{cute_cat.png}
\endminipage
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{shark.png}
\endminipage
\caption{Cat and Shark, GVision baseline}
\label{fig:cat_shark_original}
\end{figure}


Here's how \href{https://cloud.google.com/vision}{Google Cloud Vision API} classifies the original samples.

\begin{figure}[!htb]
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{cute_cat_screenshot.png}
\endminipage\hfill
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{shark_screenshot.png}
%   \caption{$\epsilon = 0.05$}\label{fig:fgsm_margin_eps_05}
\endminipage\hfill
\caption{Cat and Shark, GVision baseline}
\label{fig:cat_shark_gvision_baseline}
\end{figure}

\subsection{TREMBA}

\subsection{RayS}
This one is hard label attack and doesn't use the continuos loss from GVision.

\subsection{SquareAttack}
Because of high query intensity, we have only tested the sample cat image.


\subsubsection{SquareAttack L2}
\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_l2_7.8_iter_834.png}
\endminipage
\null\hspace{1cm}
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_l2_7.8_iter_834_screenshot.png}
\endminipage
\caption{SquareAttack, 834 queries, pertubation norm $l_2 = 7.84$ }
\label{fig:square_cat_l2}
\end{figure}

\subsubsection{SquareAttack Linf}

\begin{figure}[!htb]
\null\hspace{1cm}
\minipage{0.3\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_linf_0.047_iter_1402.png}
\endminipage
\null\hspace{1cm}
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{square_attack/gvision_cat_linf_0.047_iter_1402_screenshot.png}
\endminipage
\caption{SquareAttack, 1402 queries, pertubation norm $l_{inf} = 0.047$}
\label{fig:square_cat_linf}
\end{figure}

\subsubsection{Evaluation of the attack results}

In both L2 and Linf modes we have achived out top-1 misclassification objective and the cat label was taken in both cases to the top-3 place.

In this experiment of sample size = 1 the L2 version of SquareAttack seems to produce much less visually perceptible pertubation and is able to achive the top-1 misclassification with $l_2$ norm of only 7.84.

The Linf version on the other hand had to be given two times larger query budget, but we still had to turn up the $l_{inf}$ pertubation norm to 0.047 to achieve our misclassification objective.


This observation made us focus more on the $l_2$ bounded attacks in later local experiments.

\subsubsection{Local query distribution}





\section{Local transferability experiments}
Motivated by the not-ideal query-efficiency of pure-blackbox attacks (\ref{need_for_query_efficiency}), we moved to transfer attacks to see how far we can push the pure transfer threat model.


\subsection{Choice of local models}
We performed all our experiments on the following pretrained PyTorch ImageNet models.

\begin{itemize}
    \item ResNet-18, ResNet-50 (\cite{he2015deep})
    \item ResNeXt-50 (32x4d) (\cite{xie2017aggregated})
    \item Wide-ResNet-50-2 (\cite{zagoruyko2017wide})
    \item Squeezenet (\cite{iandola2016squeezenet})
    \item DenseNet-121 (\cite{huang2018densely})
    \item EfficientNet (\cite{tan2020efficientnet})
    \item EfficientNet adversarially trained (\cite{tramer2020ensemble})
\end{itemize}

Apart from EfficientNets, all models were taken from the \href{https://pytorch.org/vision/stable/models.html}{torchvision.models} Python package. For the EfficientNets we used \href{https://github.com/lukemelas/EfficientNet-PyTorch}{github.com/lukemelas/EfficientNet-PyTorch} reimplementation, because the original implementation uses Tensorflow, and PyTorch is just so much better than Tensorflow.

To address the particular choice of our model set, we aimed at low model size, such that forward and backward pass fits comfortably in the 2GB of our MX-150 NVIDIA laptop GPU. This requirement for instance ruled out the VGG-style networks (\cite{simonyan2015deep}), the pre-ResNet 2nd-best submission to the ILSVRC 2014 (\cite{russakovsky2015imagenet}). The Wide-ResNet-50-2 (WRN-50-2) is already at the limit with its 68,951,464 trainable parameters. In a standard FP-32 mode the model parameters, forward pass activations and backward pass partial derivatives take up almost 1GB of GPU memory. Adding to this the 620MB of GPU memory consumed by PyTorch at idle meant we could use only batch size of one for this particular Wide ResNet. But looking at the \href{https://robustbench.github.io/}{RobustBench leaderboard} (\cite{croce2021robustbench}), which uses AutoAttack (\cite{croce2020reliable}) suite of parameter-free attacks to benchmark the robustness of various adversarially robust models, one can see the top positions are dominated by WideResNets, so we kept it in our model set despite its large size.
We also chose models with the same input size, such that the same dataset preprocessing (\ref{dataset_preprocessing}) could be used for all of them and wouldn't complicate things any further. This decision meant we didn't use the popular Inception-v3 (\cite{szegedy2015rethinking}), which takes inputs of size 299x299 instead of 224x224 of all the other models.



\subsection{Choice of dataset}
We carry out all our experiments with ImageNet validation dataset. We pick out only the images containing organism using the mapping $m(c)$ mentioned in \ref{label_mapping} and deem the transfer attack successful if the target loss $\mathcal{L}_{margin}(x_{adv}, 0) < 0$, or in other words the top-1 label is an object label.

For the computation limitations reasons, each experiment was conducted with only the first 500 organism ImageNet validation images.

\subsubsection{Dataset preprocessing}
\label{dataset_preprocessing}
All the models we use accept input tensors of size $(\text{batch\_size}, 3, 224, 224)$ and they also share the same preprocessing procedure:
\begin{verbatim}
from torchvision import transforms
transform = transforms.Compose([
 transforms.Resize(256),
 transforms.CenterCrop(224),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
 ])
\end{verbatim}

Usually, this preprocessing is handled dynamically either by the dataloader or by the model itself. The user doesn't have to Resize, Crop or Normalize the inputs manually, but can pass in images of any size for inference or training.

However to speed up the experiments a bit we manually resized and center croped all the ImageNet val images, such that no dynamic resizing and cropping is needed. This saves up a surprising amount computation on our limited hardware.


\subsection{Baseline}
\begin{tabular}{ ||p{5cm}|p{3cm}|| }
  \hline
     \multicolumn{2}{|c|}{Performance on the first 500 ImageNet validation organism images} \\
 \hline
 \textbf{Model} & \textbf{Foolrate} \\
 \hline
 \hline
 Densenet-121 & 1.8\%  \\
 \hline
 EfficientNet-b0 & 1.0\% \\
 \hline
 EfficientNet-b0-advtrain & 1.2\% \\
 \hline
 EfficientNet-b4 & 0.8\% \\
 \hline
 EfficientNet-b4-advtrain & 2.4\% \\
 \hline
 ResNet-18 & 1.8\% \\
 \hline
 ResNet-50 & 1.2\% \\
 \hline
 ResNeXt-50 (32x4d) & 0.6\% \\
 \hline
 Squeezenet & 4.8\% \\
 \hline
 Wide-ResNet-50-2 & 1.6\% \\
 \hline
\end{tabular}

\paragraph{}
We can see that the models used are pretty good at distinguishing between animate and inanimate things. Any differances in the accuracy (maybe with the exception of Squeezenet) are probably due to the small test set size. When we evaluate 500 test images and get 1\% foolrate the 99\% binomial confidence interval is (0.216\%, 2.804\%)

\subsection{Whitebox attack algorithms}
In the whitebox setting we tried the following whitebox optimization algorithms:

\begin{itemize}
    \item FGSM (\cite{goodfellow2015explaining})
    \item Adaptive PGD (APGD) L2 (\cite{croce2020reliable})
    \item AdamPDG L2
\end{itemize}


\subsubsection{Fast gradient sign method (FGSM)}
This algorithm needs no introduction.
It is basically one-step gradient ascent on the sign of the cross-entropy loss $J(x)$, which would be normally used to train the classifier.

$$x_{adv} = clip(x + \epsilon \cdot sign(\nabla J(x)))$$


Careful reader might wonder: How do we compute a cross-entropy, when our local model doesn't output logits, but uses the max-logits mapping from \ref{computing_the_loss} instead? Well, we don't. We just pretend our surrogate is outputing excatly what it should and everything is fine. Figure \ref{fig:fgsm_margin} shows the transferability performance across different local models with varying amount of pertubation size $\epsilon$.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.010.png}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.050.png}
%   \caption{$\epsilon = 0.05$}\label{fig:fgsm_margin_eps_05}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_margin_eps_0.200.png}
%   \caption{$\epsilon = 0.2$}\label{fig:fgsm_margin_eps_20}
\endminipage
\caption{FGSM, margin output mapping}
\label{fig:fgsm_margin}
\end{figure}

To test the impact of different logits mapping function, we also try to pass the logits from the pretrained ImageNet classifier through the $softmax$, sum the two sets of probabilities, take the logarithm and pass that to the FGSM as an attempt to better mimick some binary animal-object classifier. The motivation here was to allow the gradient to pass through more than 2 output activations of the 1000-D ImageNet output vector, hopefullly to provide more fine-grained optimization information to the one step of FGSM.

But figure \ref{fig:fgsm_prob_sum} shows that this approach is approximately equal in transferability to the margin mapping, and for some unknown reason fails to produce whitebox adversarial images for the EfficientNets.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.010.png}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.050.png}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{transfer_attacks/transfer_heatmap/fgsm_logits_eps_0.200.png}
\endminipage
\caption{FGSM, probability sum mapping}
\label{fig:fgsm_prob_sum}
\end{figure}





\subsubsection{Adaptive PGD (APGD)}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{transfer_attacks/transfer_heatmap/apgd_l2_margin_n_iters_25_eps_10.png}
    \caption{blash bhal hlabh}
    \label{fig:apgd_margin}
\end{figure}

\subsection{Augmentation is all you need!}
\subsubsection{Guassian-noise augmentation}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{transfer_attacks/transfer_heatmap/augment_noise_apgd_l2_margin_n_iters_25_eot_iters_10_eps_10.png}
    \caption{Guassian noise augmentation}
    \label{fig:noise_augment}
\end{figure}



\subsubsection{Box-blur}
\subsubsection{Semantic-noise}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{transfer_attacks/transfer_heatmap/augment_blur_apgd_l2_margin_n_iters_50_eps_10.png}
    \caption{Guassian noise augmentation}
    \label{fig:fox_blur}
\end{figure}


\subsubsection{Elastic transformation}
\subsubsection{Affine transformation}

\section{Transferability evaluation on GVision}
TODO: just run the images against GVision

\subsection{Choice of evaluation metrics}

\subsection{Wordcloud}