\chapter{Introduction}
\label{introduction}
% \addcontentsline{toc}{chapter}{Introduction}

\section{The rise of artificial neural networks}
In recent years there has been an enormous surge in applications of artificial intelligence technologies based on neural networks to various fields. One of the most significant milestones that kickstarted today's AI revolution has undoubtedly been the year 2012. ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which hand-crafted specialized image-labeling systems have previously dominated, was won by AlexNet with its CNN architecture.

Artificial neural networks inspired by their biological analog had been known and researched for a long time. Even though they enjoyed much enthusiasm initially, there has been a time period (called an "AI winter" by some) when they had been neglected as an unpromising direction towards general intelligence. More classical approaches like SVM and rule-based AI systems showed better performance and computational efficiency on AI benchmarks of the time. 

What changed the game has been the available computational power that came with Moore's law and the usage of GPUs, mainly their parallel nature in accelerating matrix multiplication operations, that neural networks use heavily.

Another factor that helped the rise of neural networks has been the availability of large datasets like ImageNet, which contains 1,281,167 images for training and 50,000 images for validation organized in 1,000 categories. 

The availability of large amounts of labeled and unlabeled data, sometimes referred to as "Big data," is only getting better. A large part of our lives has moved to the virtual space thanks to the internet. Businesses had started to realize the value of the enormous amounts of traffic generated every day, and they are increasingly trying to figure out how to take advantage of it.

What was previously limited to academic circles had quickly become mainstream. Artificial neural networks have proven to be very versatile and have quickly been successfully applied to a wide range of problems. New neural network architectures and new training regimes allowed training deeper networks, which gave rise to a new field of machine learning called "Deep learning." 

Deep neural networks have shown state-of-the-art performance in machine translation, human voice synthesis and recognition, drug composition, particle accelerator data analysis, recommender systems, algorithmic trading, reinforcement learning, and many other areas.

\section{Concerns}
Large-scale deployment of neural network systems has been criticized by many for their inherent unexplainability. It is often hard to pinpoint why neural network behaves in some way and why it makes certain decisions. One problem is the role of training data, where possible biases may be negatively and unexpectedly reflected in the behavior of the AI system. Another problem is the performance on out-of-distribution examples, where network inference occurs on different kinds of data than used in the training stage.

Those concerns lead people to study the robustness of AI systems. It turned out that image recognition CNN networks are especially susceptible to the so-called adversarial attacks, where the input is perturbed slightly, but the output of the network changes wildly. Similar kinds of attacks have since been demonstrated in other areas like speech recognition, natural language processing, or reinforcement learning.

\section{Adversarial attacks}
This vulnerability of neural networks has led to a cat-and-mouse game of various attack strategies and following defenses proposed to mitigate them.

Neural networks can be attacked at different stages:
\begin{itemize}
    \item training,
    \item testing,
    \item deployment.
\end{itemize}

Training attacks exploit training dataset manipulation, sometimes called dataset poisoning, to change the behavior of the resulting trained network.

Testing attacks do not modify trained neural network, but often use the knowledge of the underlying architecture to craft adversarial examples which fool the system. 

Deployment attacks deal with real black-box AI systems, where the exact details of the system are usually hidden from the attacker. Nevertheless, partly because similar neural network architectures are used in the same classes of problems, some vulnerabilities in testing scenarios can still be exploited in deployment, even though the exact network parameters, architecture, and output are unknown to the attacker.

The purpose of this thesis is to explore the applicability of certain classes of testing attacks on real-world deployed AI systems. For simplicity, many kinds of State-of-the-art (SoTA) adversarial attacks have only been explored in the testing regime but have not been applied to truly black-box systems. 

The main aim of the thesis will be to test different types of testing attacks against Google Cloud Vision API, which provides its image labeling capabilities as Software-as-a-service (SaaS). Understandably, attacking an unknown system will be more challenging than attacking a known neural network in the testing stage. We will try to compare these two attack scenarios and come up with a reasonable attack performance metric. This information could prove helpful in selecting the most promising attack to a specific situation. 

Many SoTA testing attacks were not designed to attack specific deployed systems, so some attacks will need to be slightly changed to be used. We will explore different ways to modify existing attacks and evaluate them.

If service like Google Cloud Vision is found to be vulnerable, it's very likely that other SaaS providers like Amazon Rekognition or Clarifai would be as well. This fact could have a massive impact on all downstream applications using those SaaS APIs. For instance, content moderation mechanisms, which rely mainly on automatic detection, could be circumvented.

Finally, a better understanding of the failure modes of current AI systems may help us create better ones in the future. \\


The structure of this work is as follows:

In Preliminaries and related work \ref{preliminaries} we explain the most important concepts of machine and deep learning. We introduce the phenomenon of adversarial attacks and review the current progress made in the whitebox and blackbox adversarial attacks relevant to our goal.

In chapter Our approach \ref{our_approach} we put forward the motivations behind our solution and experiments. We suggest possible ways of combining current whitebox and blackbox attack methods with a goal of ultimately targeting the cloud blackbox classifiers.

In chapter Experiments \ref{experiments_chap} we present the detailed results of the conducted experiments, which are motivated in chapter \ref{our_approach}. We focus mostly on transferability of adversarial images, which will be explained in chapter \ref{related_work}, as it is one of the best ways to significantly reduce queries to the attacked target and increas the chances of staying undetected in a real-world scenario.

In Conclusion \ref{conclusion} we sum up our contributions and evaluate the degree to which we have been able to fulfill our goals stated in this chapter \ref{introduction}.

Finally, in Future work \ref{future_work} we propose a handful of potentially promising directions to explore in the future based on our results from the Experiments \ref{experiments_chap}.