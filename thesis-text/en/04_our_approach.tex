\chapter{Our approach}

\section{Adapting off-the-shelf attack algorithms to partial-information setting}
Cloud-based image classifiers don't usually classify input images into a fixed number of classes. They instead output variable-length list of probable labels with scores. And what's worse, those scores aren't even probabilities, because they don't sum up to one. 

Most of current score-based SoTA adversarial attacks assume that we have access to all output logits of the attacked target classifier. If we want to use them, we need to map somehow the cloud's variable-length score-output to fixed-length vector, which will simulate logits output of a standard CNN classifier.


\subsection{Object-organism binary classification mapping}
\label{label_mapping}
To simplify the experiments, we define a simple benchmarking attack scenario:

\begin{nscenter}
\bfseries Given an image containing a living thing, fool the target into classifying it as a non-living object.
\end{nscenter}

This choice makes our simulated classifier a binary one. It should assign each input image $(x)$ an organism score $o_{organism}(x)$ and object score $o_{object}(x)$. This 2-D score vector $(o_{object}(x), o_{organism}(x))$ is further denoted by $o(x)$ for simlicity.

We chose this split, because we perform majority of our experiments on ImageNet validation dataset (\cite{deng2009imagenet}) and ImageNet is relatively balanced between those two semantic categories. 

Why ImageNet? Despite the dataset being quite old, it is still the most heavily used dataset in the research community and the majority of freely available pretrained models are pretrained on it.

Furthermore, when $\|C\| = 2$ (where $C$ is a set of output categories), targeted and untargeted attack scenarios don't differ anymore and are neatly unified.

Adapting the attack algorithm to a different attack objective only requires swapping the label mapping layer.

\subsubsection{Imagenet category mapping}
Classic ImageNet dataset contains real-world photos, each corresponding to one and only one classification category out of 1000 possible categories. Each ImageNet category $c$ corresponds to a unique wordnet synset $w(c)$. These synsets are rather specific, but we can take a look at their set of hypernyms $h(w(c))$. If this hypernym set $h(w(c))$ contains the $organism$ synset, it should be an organism, otherwise $c$ is probably an object. 

Writen more rigorously, we map each ImageNet category $c$ into $\{organism, object\}$ using the following maping $m_{local}(c)$:

\[ m_{local}(c) = \begin{cases} 
      organism & organism \in h(w(c)) \\
      object & organism \notin h(w(c)) \\
   \end{cases}
\]


\subsubsection{Cloud label mapping}
This situation isn't so clear-cut in the case of general labels returned by cloud classifier. Labels might not even be single words, but whole sentences. We therefore resort to a more powerful label classification method and use a GPT-2 transformer (\cite{radford2019language}) for this matter. More specifically, we use the HuggingFace (\cite{wolf2020huggingfaces}) \href{https://joeddav.github.io/blog/2020/05/29/ZSL.html}{zero-shot classification pipeline} to encode text labels into embedding vector space and then compute their similarity $s(l_{cloud}, l_{gold})$ with carefully chosen set of organism labels $L_{organism} = \{animal, species\}$ and with a set of object labels $L_{object} = \{object, instrument\}$. The resulting binary cloud label mapping $m_{cloud}(l_{cloud})$ is defined as follows:

\[ m_{cloud}(l_{cloud}) = \begin{cases} 
      organism & \argmax{c \in (L_{organism} \bigcup L_{object})} s(l_{cloud}, c) \in L_{organism} \\
      object & \argmax{c \in (L_{organism} \bigcup L_{object})} s(l_{cloud}, c) \in L_{object} \\
   \end{cases}
\]

From now on, by $m(c)$ we mean either $m_{local}(c)$ or $m_{cloud}(l_{cloud})$ where the distinction wouldn't make any difference.

\subsubsection{Computing the adversarial loss}
\label{computing_the_loss}
There is one more step we have to do to transparently simulate a binary classifier with 2 output logits.

By passing the model outputs through the separation mapping $m(c)$ we obtain two score sets: $S_{organism}$ and $S_{object}$. In the local case:
$$\|S_{organism}\| + \|S_{object}\| = \|L_{\text{ImageNet}}\| = 1000$$.

In the case of a general cloud classifier these sets have variable sizes and one or both can be even empty.

There are multiple sensible ways to produce output logits vector $o(x)$ that would roughly correspond to an organism binary classifier output and which could be attacked using standard untargeted adversarial attacks.

Just to name a few choices for $o(x)$ that could intuitively work:
\begin{enumerate}
    \item top-1 score: $\max S$ 
    \item sum of logits: $\sum S$
    \item sum of un-normalized probabilities: $\sum exp(S)$
    \item log-sum of probabilities: $\log \sum softmax(S)$
    \item ...
\end{enumerate}

But in the end we want to achive a misclasification of the original organism image $x_{org}$. \\ If we go with 1) and produce output vector $o(x) = (\max S_{object}, \max S_{organism})$, misclasification is achived when $o(x)_0 > o(x)_1$. We can define a margin loss objective $\mathcal{L}_{margin}(x, \kappa) = \max S_{organism}(x) - \max S_{object}(x) + \kappa$ with additional parameter $\kappa$, by which we adjust our requirement for the degree of misclasification.

Another hint that this might be a solid choice comes from the \cite{carlini2017evaluating}, where they discuss the choice of optimization objective. They try a number of different alternatives, but in the end they conclude that optimizing the margin loss works the best, so we stick with it.


\section{PoC black-box GVision attacks}
\label{poc_gvision_attacks}
We first explored the viability and sample-efficiency of current SoTA black-box attacks. We ran the following against Google Vision API image classifier.

\begin{itemize}
    \item \ref{tremba_poc} TREMBA (\cite{Huang2020BlackBoxAA})
    \item \ref{rays_poc} RayS (\cite{Chen2020RaySAR})
    \item \ref{square_poc} SquareAttack (\cite{Andriushchenko2020SquareAA})
    \item \ref{sparse_rs_poc} Sparse-RS (\cite{Croce2020SparseRSAV})
\end{itemize}

TODO: describe each attack briefly and show sample images

\subsection{TREMBA}
\label{tremba_poc}
\subsection{RayS}
\label{rays_poc}
\subsection{SquareAttack}
\label{square_poc}
\subsection{Sparse-RS}
\label{sparse_rs_poc}

We go into more details in the Experiments \ref{experiments_chap}


\section{The need for query-efficient attack}
\label{need_for_query_efficiency}
% The are many problems with the number of queries being in the hundreds/thousands.
In the previous section \ref{poc_gvision_attacks} we empirically showed, that Google Vision API isn't 100\% robust to iterative blackbox atttacks. But although the previously mentioned blackbox attacks are often successful in producing adversarial image, query count to the target may be often unacceptably high. Huge query stress to the target is troublesome for several reasons:

\begin{itemize}
    \item{High cost - 1.5\$ per 1000 queries}
    \item{Raising suspicion}
    \item{Often unrealistic in practical setting}
\end{itemize}


The problem is that these blackbox attacks (with the exception of TREMBA) mostly rely on random search and don't make use of the gradient similarity of various CNN models. The high dimensionality of the input data doesn't make the blackbox optimization task easy. Current SoTA blackbox attacks that don't use any gradient priors are already at the query efficiency limit with their medium queries being often less than 100 (but that of course depends on the precise threat-model under which the attack is evaluated). Even though the median in the hundreds is amazing when compared to early attempts like ZOO (\cite{Chen2017ZOOZO}) which required queries on the order of $10^4$, it is still not satisfactory for a practical use. 


\section{Leveraging transferability}
\label{Leveraging_transferability}
After these early experiments that proved the concept, we focused our attention on transferability, which has a huge potential to save queries.

\subsection{Transfer attacks provide better seeds for blackbox optimization}
\label{transfer_attack_seeds}
Even if the localy-produced adversarial images don't transfer directly to the target, \cite{Suya2020HybridBA} showed, that the output of transfer-attack can provide better starting seeds for blackbox optimization attacks and improve their query efficiency. The option to choose from several starting points basically adds a degree of freedom to the blackbox optimization. They also discuss different prioritization strategies, as the the number of seeds produced isn't limited by target queries and we can therefore afford to produce as many candidate starting points as we like.

\subsection{Train, validate, test}
There is a weak analogy between the crafting of an adversarial example and the training of machine learning model. ML model weights are first fitted againt specified loss contraint. This constraint is (among other things) a function of training data. The weights are then validated and checked against overfitting on a slightly different constraint, which now depends on a validation dataset. When all is good, model is happily deployed to production.

With a bit of imagination, ML model weights correspond to pixel values of an adversarial image. The pixels are first trained by gradient descent on training loss provided by a surrogate model. They are validated against ensemble set of diverse indepenent classifiers, and when the foolrate is good, they are sent for test evaluation to the cloud. 

\subsection{Local training and validation is cheap}
We want to offload the cloud query-stress to local simulation as much as possible. An attacker can often afford to spend orders of magnitude more queries to local surrogates and validation models than to the actual target.

\subsection{Multiple candidates save queries}
Iterative black-box attacks usually have query-distrubutions which are tail-heavy. In other words, the median queries needed to create a successful adversarial image are much lower than the average queries.

Let's image an attack scenario, where we want to submit a photo to a platform with automatic content moderation mechanism. Querying the target hundreds of times would certainly attract unwanted attention and our heavy queries can quickly trigger human evaluation. If our primary goal is to craft only one adversarial image and as much as possible evade detection, having multiple candidate images would give us another degree of freedom and it could potentially mitigate the heavy-tail problem. This approach can be in principle transparently combined with the multiple-seed candidate suggestions mentioned in \ref{Leveraging_transferability} using the same prioritization candidate scoring mechanism.


\section{The need for attack pipeline}
We argued in \ref{Leveraging_transferability} that combining multiple whitebox and blackbox attack approaches could create more powerful attack as well as giving us more freedom and flexibility to tailor this combination to a specific attack scenario constraints. As of now, there isn't any general whitebox/blackbox attack pipeline which would combine different algorithms and allow us attacking cloud services in a practical way.

\subsection{Possibility of multiple blackbox workers}
We can also image running multiple different attacks in paralel and having some meta-controller orchestrating individual attack algorithms such that we minimize queries to the target and efficiently make use of the additional degrees of freedom.


\subsection{The need for unified attack and model API}
There are sereval frameworks unifing whitebox/blackbox attacks. To name a few, there is 
FoolBox (\cite{Rauber2020FoolboxNF}) or AutoAttack (\cite{croce2020reliable}). 

Although they are excellent at testing the robustness of local models, they don't give us the flexibility we need to implement all the pipeline features mentioned previously. They cannot be used without some modification to attack cloud models and their optimization attacks cannot be cooperatively scheduled step by step, which is what would be required for effective multi-attack orchestration.

\section{AdvPipe}

\subsection{The vision}
Solves all our problems. At least in theory.

\subsection{The reality}
Solves some of our problems. Like 10\%.

TODO: Make some excuses why you didn't make it in time.

TODO: Make some flowchart of what is actually working.


\section{Implementation}
AdvPipe is implemented in Python 3.8 and uses primarily PyTorch 1.9.0 (\cite{paszke2019pytorch}) and NVIDIA Cuda as the computational backend.

\subsection{Attack regimes}
\subsubsection{Cooperative iterative regime}
\subsubsection{Transfer regime}
\subsubsection{Transfer regime with multiple targets}


\subsection{Attack algorithms}
\subsubsection{Whitebox}
\subsubsection{Blackbox}

\subsection{Wrapping whitebox and blackbox models}
All models (cloud, local) are wrapped as PyTorch \codeword{torch.nn.Modules}. This way they can be used in a plug-and-play manner and passed easily to existing attack algorithms or frameworks like FoolBox (\cite{Rauber2020FoolboxNF}). Model outputs are mapped to a 2-D score vector using the mapping discussed in \ref{label_mapping}, which allows us to optimize our custom binary objective.

Cloud models are compatible with any blackbox attacks, as long as the attacks don't require the gradients (which they shouldn't anyway).

Local models are on the other hand fully differentiable, so we can attack them with whitebox attacks on top of the standard blackbox attacks. 

\subsubsection{Preprocessing}
In chapter \ref{experiments_chap} we thoroughly explore the effects of different augmentation and regularization techniques to enhance transferability. These are often implemented as stochastic preprocessing layers which are differentiable. For this purpose we mostly use Kornia \textendash \space differentiable computer vision library for PyTorch (\cite{riba2019kornia}).

\subsection{Configuration}
AdvPipe is highly configurable by using YAML config files. These are parsed and checked by \codeword{config\_datamodel.py}.


\subsubsection{Config templating}
The YAML configs files also support simple templating mechanism, which can be used to run multiple experiments with slightly different hyperparameters. This is in line with the DRY pricinple and helps to keep the number of configuration files in sane numbers.

\subsection{Dependency management}
The motivation to run our framework on a number of differrent machines with different environments, easily installing AdvPipe reliably and escaping dependency hell became our priority number one. We needed to accomodate Tensorflow (required by the HuggingFace pipeline) and PyTorch and a number of other dependencies in the same Python environment. We decided to do away with \href{https://pip.pypa.io/en/stable/}{pip}, that can often leave the Python virtual environment in an inconsistent state and also with conda environment manager (\cite{anaconda}), which ensures package compatibily, but doesn't always contain all the latest Python package versions. We instead moved to \href{https://python-poetry.org/}{Poetry} - an excellent pip alternative - to manage our dependencies.
